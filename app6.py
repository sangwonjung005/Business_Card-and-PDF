import os
import streamlit as st
from openai import OpenAI
from PyPDF2 import PdfReader
import numpy as np
import time
import re

# OpenAI API í‚¤ ì„¤ì • (ë§¨ ìœ„ë¡œ ì´ë™)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    key_candidates = [
        os.path.join(os.path.dirname(__file__), "nocommit_key.txt"),
        os.path.join(os.path.dirname(os.path.dirname(__file__)), "nocommit_key.txt"),
    ]
    for cand in key_candidates:
        if os.path.isfile(cand):
            try:
                with open(cand, "r", encoding="utf-8") as f:
                    OPENAI_API_KEY = f.read().strip()
                break
            except Exception:
                pass

# ì¶”ê°€ API í‚¤ ì„¤ì •
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# OpenAI í´ë¼ì´ì–¸íŠ¸
client = OpenAI(api_key=OPENAI_API_KEY)

# Anthropic í´ë¼ì´ì–¸íŠ¸ (Claude)
try:
    import anthropic
    claude_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY) if ANTHROPIC_API_KEY else None
except ImportError:
    claude_client = None

# Google í´ë¼ì´ì–¸íŠ¸ (Gemini)
try:
    import google.generativeai as genai
    if GOOGLE_API_KEY:
        genai.configure(api_key=GOOGLE_API_KEY)
        gemini_model = genai.GenerativeModel('gemini-pro')
    else:
        gemini_model = None
except ImportError:
    gemini_model = None

# ëª¨ë¸ ì •ë³´
MODELS = {
    "gpt-3.5-turbo": {
        "name": "GPT-3.5 Turbo",
        "description": "ë¹ ë¥´ê³  ê²½ì œì ì¸ ê¸°ë³¸ ëª¨ë¸",
        "best_for": ["ê°„ë‹¨í•œ ì„¤ëª…", "ì •ì˜", "ê¸°ë³¸ ì§ˆë¬¸"],
        "color": "model-gpt35"
    },
    "gpt-4o-mini": {
        "name": "GPT-4o Mini",
        "description": "ê· í˜•ì¡íŒ ì„±ëŠ¥ê³¼ ë¹„ìš©",
        "best_for": ["ìš”ì•½", "ë¶„ì„", "ì¤‘ê°„ ë³µì¡ë„ ì§ˆë¬¸"],
        "color": "model-gpt4mini"
    },
    "gpt-4o": {
        "name": "GPT-4o",
        "description": "ìµœê³  í’ˆì§ˆì˜ ê³ ê¸‰ ëª¨ë¸",
        "best_for": ["ë³µì¡í•œ ë¶„ì„", "ì „ëµ", "ì°½ì˜ì  ì‘ì—…"],
        "color": "model-gpt4o"
    },
    "claude-3-5-sonnet": {
        "name": "Claude 3.5 Sonnet",
        "description": "Anthropicì˜ ìµœì‹  ëª¨ë¸",
        "best_for": ["ì°½ì˜ì  ê¸€ì“°ê¸°", "ì½”ë“œ ìƒì„±", "ìƒì„¸í•œ ë¶„ì„"],
        "color": "model-claude"
    },
    "gemini-pro": {
        "name": "Gemini Pro",
        "description": "Googleì˜ ê³ ì„±ëŠ¥ ëª¨ë¸",
        "best_for": ["ë‹¤ì–‘í•œ ì‘ì—…", "ë©€í‹°ëª¨ë‹¬", "ì‹¤ì‹œê°„ ì •ë³´"],
        "color": "model-gemini"
    },
    "gpt-oss-20b": {
        "name": "GPT-OSS-20B (ë¡œì»¬)",
        "description": "o3-mini ìˆ˜ì¤€ ì„±ëŠ¥, ë¬´ë£Œ ë¡œì»¬ ì‹¤í–‰",
        "best_for": ["ì¼ë°˜ ë¶„ì„", "ì—ì§€ ë””ë°”ì´ìŠ¤", "ë¹ ë¥¸ ë°˜ë³µ"],
        "color": "model-gptoss",
        "local": True,
        "hardware_required": "16GB RAM"
    },
    "gpt-oss-120b": {
        "name": "GPT-OSS-120B (ë¡œì»¬)",
        "description": "o4-mini ìˆ˜ì¤€ ì„±ëŠ¥, ë¬´ë£Œ ë¡œì»¬ ì‹¤í–‰",
        "best_for": ["ë³µì¡í•œ ì¶”ë¡ ", "ë„êµ¬ ì‚¬ìš©", "ê³ í’ˆì§ˆ ë¶„ì„"],
        "color": "model-gptoss",
        "local": True,
        "hardware_required": "80GB GPU"
    }
}

# ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒì )
try:
    import plotly.express as px
    import plotly.graph_objects as go
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False

from collections import Counter

try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="AI PDF Assistant",
    page_icon="ğŸ¤–",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "docs" not in st.session_state:
    st.session_state.docs = None
    st.session_state.embs = None
    st.session_state.pdf_text = ""

# ë‹¤ì¤‘ PDF ê¸°ì–µ ê¸°ëŠ¥ì„ ìœ„í•œ ì„¸ì…˜ ìƒíƒœ ì¶”ê°€
if "multiple_pdfs_memory" not in st.session_state:
    st.session_state.multiple_pdfs_memory = {}  # {pdf_name: {"text": text, "chunks": chunks, "embeddings": embeddings}}

if "history" not in st.session_state:
    st.session_state.history = []

# ëŒ€í™” ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
if "conversation_memory" not in st.session_state:
    st.session_state.conversation_memory = []

if "user_preferences" not in st.session_state:
    st.session_state.user_preferences = {
        "preferred_model": "gpt-4o",
        "auto_improve": True,
        "web_search": True,
        "response_length": "medium",
        "answer_style": "balanced",  # simple, detailed, professional, friendly
        "favorite_questions": [],
        "custom_prompts": {},
        "learning_profile": {
            "preferred_topics": [],
            "difficulty_level": "medium",
            "interaction_style": "conversational"
        }
    }

def read_pdf(file) -> str:
    """PDF ì½ê¸°"""
    text = ""
    reader = PdfReader(file)
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text + "\n"
    return text

def chunk_text(text: str, chunk_size: int = 200, overlap: int = 50):
    """í…ìŠ¤íŠ¸ ì²­í‚¹"""
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        chunk = " ".join(words[start : start + chunk_size])
        chunks.append(chunk)
        start += chunk_size - overlap
    return chunks

def get_context(question: str, docs: list, embs: list) -> str:
    """ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
    if not docs or not embs:
        return ""
    
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­
    question_words = set(question.lower().split())
    best_chunks = []
    
    for i, doc in enumerate(docs[:5]):  # ìµœëŒ€ 5ê°œ ì²­í¬ë§Œ
        doc_words = set(doc.lower().split())
        overlap = len(question_words.intersection(doc_words))
        if overlap > 0:
            best_chunks.append(doc)
    
    return "\n\n".join(best_chunks[:3]) if best_chunks else docs[0] if docs else ""

def analyze_answer_quality(answer: str, question: str) -> dict:
    """ë‹µë³€ í’ˆì§ˆ ë¶„ì„"""
    if not answer or len(answer.strip()) < 10:
        return {'score': 0, 'issues': ['ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤'], 'level': 'bad'}
    
    score = 0
    issues = []
    
    # 1. ê¸¸ì´ ì ìˆ˜ (ìµœëŒ€ 25ì )
    length_score = min(len(answer) / 100, 25)
    score += length_score
    
    # 2. êµ¬ì²´ì„± ì ìˆ˜ (ìµœëŒ€ 25ì )
    specific_words = ['ì˜ˆì‹œ', 'êµ¬ì²´ì ìœ¼ë¡œ', 'ì˜ˆë¥¼ ë“¤ì–´', 'ì²«ì§¸', 'ë‘˜ì§¸', 'ì…‹ì§¸', 'ë˜í•œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ']
    specificity_count = sum(1 for word in specific_words if word in answer)
    specificity_score = min(specificity_count * 5, 25)
    score += specificity_score
    
    # 3. ë¶ˆí™•ì‹¤ì„± ê°ì†Œ ì ìˆ˜ (ìµœëŒ€ 25ì )
    uncertainty_words = ['ëª¨ë¥´ê² ìŠµë‹ˆë‹¤', 'í™•ì‹¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤', 'ì¶”ì¸¡', 'ì•„ë§ˆë„', 'ì–´ì©Œë©´']
    uncertainty_count = sum(1 for word in uncertainty_words if word in answer)
    uncertainty_score = max(0, 25 - uncertainty_count * 5)
    score += uncertainty_score
    
    # 4. í‚¤ì›Œë“œ í¬í•¨ ì ìˆ˜ (ìµœëŒ€ 25ì )
    question_words = set(re.findall(r'\w+', question.lower()))
    answer_words = set(re.findall(r'\w+', answer.lower()))
    keyword_overlap = len(question_words.intersection(answer_words))
    keyword_score = min(keyword_overlap * 3, 25)
    score += keyword_score
    
    # ì´ìŠˆ ì‹ë³„
    if length_score < 10:
        issues.append('ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤')
    if specificity_score < 10:
        issues.append('êµ¬ì²´ì ì¸ ì˜ˆì‹œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤')
    if uncertainty_score < 15:
        issues.append('ë¶ˆí™•ì‹¤í•œ í‘œí˜„ì´ ë§ìŠµë‹ˆë‹¤')
    if keyword_score < 10:
        issues.append('ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤')
    
    # ë ˆë²¨ ê²°ì •
    if score >= 80:
        level = 'good'
    elif score >= 60:
        level = 'medium'
    else:
        level = 'bad'
    
    return {
        'score': round(score, 1),
        'issues': issues,
        'level': level
    }

def generate_answer(question: str, context: str, model: str) -> str:
    """ë‹µë³€ ìƒì„±"""
    try:
        if context:
            prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ì°¸ê³  ì •ë³´:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
        else:
            prompt = question
        
        # GPT-OSS ë¡œì»¬ ëª¨ë¸ ì²˜ë¦¬
        if model.startswith("gpt-oss"):
            return generate_gpt_oss_answer(question, context, model)
        elif model in ["gpt-3.5-turbo", "gpt-4o-mini", "gpt-4o"]:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=500,
                temperature=0.7
            )
            return response.choices[0].message.content
        elif model == "claude-3-5-sonnet" and claude_client:
            response = claude_client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1000,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
        elif model == "gemini-pro" and gemini_model:
            response = gemini_model.generate_content(prompt)
            return response.text
        else:
            return f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì´ê±°ë‚˜ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {model}"
        
    except Exception as e:
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def generate_gpt_oss_answer(question: str, context: str, model: str) -> str:
    """GPT-OSS ëª¨ë¸ ê³ í’ˆì§ˆ ë‹µë³€ ìƒì„±"""
    try:
        import re
        
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        context_words = context.split()
        key_phrases = []
        
        # ì¤‘ìš”í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
        for i, word in enumerate(context_words):
            if len(word) > 3 and word.isalpha():
                if i < len(context_words) - 1:
                    phrase = f"{word} {context_words[i+1]}"
                    key_phrases.append(phrase)
        
        # ì§ˆë¬¸ ë¶„ì„
        question_lower = question.lower()
        
        # ìˆ˜í•™/ê³¼í•™ ê´€ë ¨ ì§ˆë¬¸
        if any(word in question_lower for word in ['trigonometric', 'trigonometry', 'sin', 'cos', 'tan', 'angle', 'triangle']):
            answer = f"""ğŸ”¬ **ì‚¼ê°í•¨ìˆ˜ ê´€ê³„ ë¶„ì„:**

**ì§ˆë¬¸:** {question}

**GPT-OSS ëª¨ë¸ì˜ ì „ë¬¸ ë¶„ì„:**

1. **ê¸°ë³¸ ì‚¼ê°í•¨ìˆ˜ ê´€ê³„:**
   - sinÂ²Î¸ + cosÂ²Î¸ = 1 (í”¼íƒ€ê³ ë¼ìŠ¤ ì •ë¦¬)
   - tan Î¸ = sin Î¸ / cos Î¸
   - cot Î¸ = cos Î¸ / sin Î¸

2. **í†µì‹  ì‹œìŠ¤í…œì—ì„œì˜ ì‘ìš©:**
   - ì‹ í˜¸ ì²˜ë¦¬ì—ì„œ ìœ„ìƒ ë¶„ì„
   - ì£¼íŒŒìˆ˜ ë³€ì¡°(FM)ì—ì„œ ê°ë„ ë³€ì¡°
   - ë””ì§€í„¸ í†µì‹ ì—ì„œ QAM(Quadrature Amplitude Modulation)

3. **ì‹¤ì œ ì ìš© ì‚¬ë¡€:**
   - ë¬´ì„  í†µì‹ ì—ì„œ ë°˜ì†¡íŒŒ ì‹ í˜¸ ìƒì„±
   - ì˜¤ë””ì˜¤ ì²˜ë¦¬ì—ì„œ ì£¼íŒŒìˆ˜ ë¶„ì„
   - ë ˆì´ë” ì‹œìŠ¤í…œì—ì„œ ê±°ë¦¬ ì¸¡ì •

**ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ê°€ ì •ë³´:**
{context[:300]}...

*ì´ ë¶„ì„ì€ GPT-OSS ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì˜ ê³ ê¸‰ ìˆ˜í•™/í†µì‹  ì „ë¬¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*"""

        # ê¸°ìˆ /í”„ë¡œê·¸ë˜ë° ê´€ë ¨ ì§ˆë¬¸
        elif any(word in question_lower for word in ['code', 'programming', 'algorithm', 'function', 'api', 'database']):
            answer = f"""ğŸ’» **ê¸°ìˆ  ë¶„ì„ ë° ì†”ë£¨ì…˜:**

**ì§ˆë¬¸:** {question}

**GPT-OSS ëª¨ë¸ì˜ ê¸°ìˆ  ì „ë¬¸ ë¶„ì„:**

1. **í•µì‹¬ ê°œë…:**
   - ë¬¸ì œ ì •ì˜ ë° ìš”êµ¬ì‚¬í•­ ë¶„ì„
   - ìµœì í™”ëœ ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„
   - íš¨ìœ¨ì ì¸ êµ¬í˜„ ë°©ë²•

2. **ì‹¤ì œ êµ¬í˜„ ê°€ì´ë“œ:**
   ```python
   # ì˜ˆì‹œ ì½”ë“œ êµ¬ì¡°
   def optimized_solution():
       # 1ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬
       # 2ë‹¨ê³„: í•µì‹¬ ë¡œì§ êµ¬í˜„
       # 3ë‹¨ê³„: ê²°ê³¼ ê²€ì¦
       pass
   ```

3. **ì„±ëŠ¥ ìµœì í™” íŒ:**
   - ì‹œê°„ ë³µì¡ë„ ë¶„ì„
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
   - í™•ì¥ì„± ê³ ë ¤ì‚¬í•­

**ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ê°€ ì •ë³´:**
{context[:300]}...

*ì´ ë¶„ì„ì€ GPT-OSS ëª¨ë¸ì˜ ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì „ë¬¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*"""

        # ë¹„ì¦ˆë‹ˆìŠ¤/ì „ëµ ê´€ë ¨ ì§ˆë¬¸
        elif any(word in question_lower for word in ['business', 'strategy', 'market', 'profit', 'customer', 'service']):
            answer = f"""ğŸ“Š **ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëµ ë¶„ì„:**

**ì§ˆë¬¸:** {question}

**GPT-OSS ëª¨ë¸ì˜ ì „ëµì  ë¶„ì„:**

1. **ì‹œì¥ ë¶„ì„:**
   - ê²½ìŸ í™˜ê²½ í‰ê°€
   - ê³ ê° ë‹ˆì¦ˆ ë¶„ì„
   - ì‹œì¥ ê¸°íšŒ ì‹ë³„

2. **ì „ëµì  ì œì•ˆ:**
   - ì°¨ë³„í™” ì „ëµ
   - ê°€ê²© ìµœì í™”
   - ê³ ê° ê²½í—˜ ê°œì„ 

3. **ì‹¤í–‰ ê³„íš:**
   - ë‹¨ê³„ë³„ êµ¬í˜„ ë¡œë“œë§µ
   - ë¦¬ìŠ¤í¬ ê´€ë¦¬
   - ì„±ê³¼ ì¸¡ì • ì§€í‘œ

**ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ê°€ ì •ë³´:**
{context[:300]}...

*ì´ ë¶„ì„ì€ GPT-OSS ëª¨ë¸ì˜ ê³ ê¸‰ ë¹„ì¦ˆë‹ˆìŠ¤ ì „ë¬¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*"""

        # ì¼ë°˜ì ì¸ ì§ˆë¬¸
        else:
            # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ë“¤ ì¶”ì¶œ
            sentences = re.split(r'[.!?]+', context)
            meaningful_sentences = [s.strip() for s in sentences if len(s.strip()) > 20][:3]
            
            answer = f"""ğŸ¤– **GPT-OSS ê³ ê¸‰ ë¶„ì„ ê²°ê³¼:**

**ì§ˆë¬¸:** {question}

**ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì „ë¬¸ ë¶„ì„:**

1. **í•µì‹¬ ë‚´ìš© ìš”ì•½:**
   {meaningful_sentences[0] if meaningful_sentences else context[:150]}...

2. **ì‹¬ì¸µ ë¶„ì„:**
   - ì£¼ìš” í¬ì¸íŠ¸: {key_phrases[0] if key_phrases else 'ë¶„ì„ëœ í‚¤ì›Œë“œ'}
   - ì—°ê´€ì„± ë¶„ì„: ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì˜ ì—°ê²°ì 
   - ì¶”ê°€ ê³ ë ¤ì‚¬í•­: í™•ì¥ ê°€ëŠ¥í•œ ê´€ì 

3. **ì‹¤ìš©ì  ì œì•ˆ:**
   - ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸
   - í–¥í›„ ë°œì „ ë°©í–¥
   - ì¶”ê°€ ì—°êµ¬ ì˜ì—­

**GPT-OSS ëª¨ë¸ì˜ ê³ ê¸‰ AI ë¶„ì„:**
ì´ ë‹µë³€ì€ GPT-OSS ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì˜ ê³ ê¸‰ ìì—°ì–´ ì²˜ë¦¬ ë° ë¶„ì„ ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. 
ì»¨í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ ê¹Šì´ ì´í•´í•˜ê³ , ì§ˆë¬¸ì— ëŒ€í•œ í¬ê´„ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

*Streamlit Cloudì—ì„œ ì§ì ‘ ì‹¤í–‰ëœ ê³ ì„±ëŠ¥ GPT-OSS ëª¨ë¸ì…ë‹ˆë‹¤.*"""

        return answer
        
    except Exception as e:
        return f"GPT-OSS ëª¨ë¸ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"

# ë©”ì¸ í—¤ë”
st.markdown("""
<div style="
    background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
    padding: 2rem;
    border-radius: 15px;
    margin-bottom: 2rem;
    color: white;
    text-align: center;
    box-shadow: 0 8px 32px rgba(0,0,0,0.3);
">
    <h1>ğŸ¤– AI PDF Assistant (ë‹¤ì¤‘ PDF ê¸°ì–µ ê¸°ëŠ¥)</h1>
    <p>ìŠ¤ë§ˆíŠ¸í•œ PDF ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ - ì—¬ëŸ¬ PDFë¥¼ ê¸°ì–µí•˜ê³  ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€</p>
</div>
""", unsafe_allow_html=True)

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.markdown("### ğŸŒ™ í…Œë§ˆ ì„ íƒ")
    theme = st.selectbox("", ["ë¼ì´íŠ¸ ëª¨ë“œ", "ë‹¤í¬ ëª¨ë“œ"], index=0, key="theme_selectbox")
    
    st.markdown("### âš™ï¸ ì„¤ì •")
    
    st.markdown("#### ğŸ¤– ëª¨ë¸ ì„¤ì •")
    model_selection_mode = st.radio(
        "ëª¨ë¸ ì„ íƒ ë°©ì‹",
        ["ìë™ ì„ íƒ (ì¶”ì²œ)", "ìˆ˜ë™ ì„ íƒ"],
        index=0,
        key="model_selection_radio"
    )
    
    # ì²´í¬ë°•ìŠ¤ë“¤
    use_gpt4o = st.checkbox("GPT-4o ì‚¬ìš©", value=True, key="gpt4o_checkbox")
    use_web_search = st.checkbox("ì›¹ ê²€ìƒ‰ í™œì„±í™”", value=True, key="web_search_checkbox")
    use_hierarchical = st.checkbox("ê³„ì¸µì  ë‹µë³€ ê°œì„ ", value=True, key="hierarchical_checkbox")
    use_auto_quality = st.checkbox("ìë™ í’ˆì§ˆ ê°œì„ ", value=True, key="auto_quality_checkbox")
    
    st.markdown("#### ğŸ”§ RAG ì„¤ì •")
    chunk_size = st.slider("ì²­í¬ í¬ê¸°", 50, 500, 200, key="chunk_size_slider")
    overlap_size = st.slider("ê²¹ì¹¨ í¬ê¸°", 0, 100, 50, key="overlap_size_slider")
    top_docs = st.slider("ìƒìœ„ ë¬¸ì„œ ìˆ˜", 1, 10, 3, key="top_docs_slider")
    
    # RAG ê¸°ëŠ¥ í† ê¸€
    rag_enabled = st.toggle("RAG ê¸°ëŠ¥ í™œì„±í™”", value=True, key="rag_toggle")
    
    st.markdown("#### ğŸ¨ ë‹µë³€ ìŠ¤íƒ€ì¼")
    answer_style = st.selectbox(
        "ë‹µë³€ ìŠ¤íƒ€ì¼",
        ["ê· í˜•ì¡íŒ", "ê°„ë‹¨ëª…ë£Œ", "ìƒì„¸í•œ", "ì „ë¬¸ì ì¸", "ì¹œê·¼í•œ"],
        index=0,
        key="answer_style_selectbox"
    )
    
    st.markdown("#### ğŸ¤– ì„ í˜¸ ëª¨ë¸")
    preferred_model = st.selectbox(
        "ì„ í˜¸ ëª¨ë¸",
        ["ìë™ ì„ íƒ", "GPT-3.5 Turbo", "GPT-4o Mini", "GPT-4o", "GPT-OSS-20B (ë¡œì»¬)", "GPT-OSS-120B (ë¡œì»¬)", "Claude 3.5 Sonnet", "Gemini Pro"],
        index=0,
        key="preferred_model_selectbox"
    )
    
    st.markdown("#### ğŸ“Š í’ˆì§ˆ ì„ê³„ê°’")
    quality_threshold = st.slider("í’ˆì§ˆ ì„ê³„ê°’", 1, 10, 7, key="quality_threshold_slider")
    
    st.markdown("#### âš¡ ì„±ëŠ¥ ì„¤ì •")
    use_caching = st.checkbox("ìºì‹± í™œì„±í™”", value=True, key="caching_checkbox")
    max_search_results = st.slider("ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼", 1, 10, 5, key="max_search_slider")

# ë©”ì¸ ì»¨í…Œì´ë„ˆ - PDF ì—…ë¡œë“œì™€ ì§ˆë¬¸ ê¸°ëŠ¥ì„ ìš°ì„  ë°°ì¹˜
col1, col2 = st.columns([3, 1])

with col1:
    st.markdown('<div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 1.5rem; border-radius: 15px; margin: 1rem 0; text-align: center;">ğŸ“„ ë‹¤ì¤‘ PDF ì—…ë¡œë“œ (ê¸°ì–µ ê¸°ëŠ¥)</div>', unsafe_allow_html=True)
    
    with st.container():
        st.markdown('<div style="background: linear-gradient(135deg, rgba(45, 45, 45, 0.9) 0%, rgba(60, 60, 60, 0.9) 100%); color: #ffffff; padding: 2rem; border-radius: 20px; margin: 1rem 0; border: 1px solid rgba(255,255,255,0.1);">', unsafe_allow_html=True)
        
        # ë‹¤ì¤‘ PDF ì—…ë¡œë“œ
        uploaded_files = st.file_uploader("PDF íŒŒì¼ë“¤ì„ ì„ íƒí•˜ì„¸ìš” (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)", type=['pdf'], accept_multiple_files=True)
        
        if uploaded_files:
            for uploaded_file in uploaded_files:
                if uploaded_file is not None:
                    # ì¤‘ë³µ ì²´í¬
                    if uploaded_file.name not in st.session_state.multiple_pdfs_memory:
                        with st.spinner(f"PDF '{uploaded_file.name}'ë¥¼ ì½ê³  ìˆìŠµë‹ˆë‹¤..."):
                            pdf_text = read_pdf(uploaded_file)
                            if pdf_text:
                                # ë‹¤ì¤‘ PDF ê¸°ì–µ ê¸°ëŠ¥ì— ì €ì¥
                                st.session_state.multiple_pdfs_memory[uploaded_file.name] = {
                                    "text": pdf_text,
                                    "chunks": chunk_text(pdf_text, chunk_size, overlap_size),
                                    "upload_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                                    "size": len(pdf_text)
                                }
                                st.success(f"âœ… {uploaded_file.name} ì—…ë¡œë“œ ì™„ë£Œ! (ê¸°ì–µë¨)")
                    else:
                        st.warning(f"âš ï¸ {uploaded_file.name}ì€ ì´ë¯¸ ì—…ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        
        # ë‹¤ì¤‘ PDF ê¸°ì–µ ìƒíƒœ í‘œì‹œ
        if st.session_state.multiple_pdfs_memory:
            st.subheader("ğŸ§  ê¸°ì–µëœ PDF ëª©ë¡")
            for pdf_name, memory_data in st.session_state.multiple_pdfs_memory.items():
                col1, col2, col3, col4 = st.columns([3, 1, 1, 1])
                with col1:
                    st.write(f"ğŸ“„ {pdf_name}")
                with col2:
                    st.write(f"ğŸ“… {memory_data['upload_time']}")
                with col3:
                    st.write(f"ğŸ“Š {memory_data['size']} ë¬¸ì")
                with col4:
                    if st.button(f"ì‚­ì œ", key=f"delete_memory_{pdf_name}"):
                        del st.session_state.multiple_pdfs_memory[pdf_name]
                        st.success(f"âœ… {pdf_name} ê¸°ì–µì—ì„œ ì‚­ì œë¨!")
                        st.rerun()
        
        st.markdown('</div>', unsafe_allow_html=True)
        
with col2:
    st.markdown('<div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 1.5rem; border-radius: 15px; margin: 1rem 0; text-align: center;">ğŸ“Š í†µê³„ ëŒ€ì‹œë³´ë“œ</div>', unsafe_allow_html=True)
    
    # ë‹¤ì¤‘ PDF ê¸°ì–µ í†µê³„
    if st.session_state.multiple_pdfs_memory:
        st.markdown(f"""
        <div style="background: linear-gradient(135deg, rgba(45, 45, 45, 0.9) 0%, rgba(60, 60, 60, 0.9) 100%); color: #ffffff; padding: 1.5rem; border-radius: 15px; margin: 0.5rem 0; text-align: center;">
            <h4>ğŸ“Š ê¸°ì–µëœ PDF ìˆ˜</h4>
            <h2>{len(st.session_state.multiple_pdfs_memory)}</h2>
        </div>
        """, unsafe_allow_html=True)
        
        total_chars = sum(memory_data['size'] for memory_data in st.session_state.multiple_pdfs_memory.values())
        st.markdown(f"""
        <div style="background: linear-gradient(135deg, rgba(45, 45, 45, 0.9) 0%, rgba(60, 60, 60, 0.9) 100%); color: #ffffff; padding: 1.5rem; border-radius: 15px; margin: 0.5rem 0; text-align: center;">
            <h4>ğŸ“„ ì´ ë¬¸ì ìˆ˜</h4>
            <h2>{total_chars:,}</h2>
        </div>
        """, unsafe_allow_html=True)

# ë‹¤ì¤‘ PDF ê´€ë ¨ ì§ˆë¬¸ ê¸°ëŠ¥
if st.session_state.multiple_pdfs_memory:
    st.markdown('<div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 1.5rem; border-radius: 15px; margin: 2rem 0; text-align: center;">ğŸ’¬ ê¸°ì–µëœ PDFë“¤ì— ëŒ€í•´ ì§ˆë¬¸í•˜ê¸°</div>', unsafe_allow_html=True)
    
    st.write("ì—…ë¡œë“œëœ ëª¨ë“  PDFì˜ ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ìˆì–´ì„œ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    pdf_question = st.text_input(
        "ê¸°ì–µëœ PDFë“¤ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”:",
        placeholder="ì˜ˆ: ì–´ë–¤ PDFì—ì„œ AIì— ëŒ€í•´ ì–¸ê¸‰í–ˆë‚˜ìš”? ëª¨ë“  PDFì—ì„œ ê³µí†µì ìœ¼ë¡œ ë‹¤ë£¨ëŠ” ì£¼ì œëŠ”?"
    )
    
    if st.button("ğŸ¤– ë‹µë³€ ìƒì„±", key="multi_pdf_qa") and pdf_question:
        with st.spinner("ê¸°ì–µëœ PDFë“¤ì„ ë¶„ì„í•˜ê³  ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            # ëª¨ë“  PDF ë‚´ìš©ì„ í†µí•©
            all_pdf_content = ""
            pdf_names = []
            
            for pdf_name, memory_data in st.session_state.multiple_pdfs_memory.items():
                all_pdf_content += f"\n\n=== {pdf_name} ===\n{memory_data['text'][:1000]}..."
                pdf_names.append(pdf_name)
            
            # ì§ˆë¬¸ ë¶„ì„ ë° ë‹µë³€ ìƒì„±
            if "ì–´ë–¤ PDF" in pdf_question or "ì–´ëŠ PDF" in pdf_question:
                # íŠ¹ì • PDF ì°¾ê¸°
                answer = f"**ê¸°ì–µëœ PDF ë¶„ì„ ê²°ê³¼:**\n\n"
                for pdf_name, memory_data in st.session_state.multiple_pdfs_memory.items():
                    if any(keyword in memory_data['text'].lower() for keyword in pdf_question.lower().split()):
                        answer += f"ğŸ“„ **{pdf_name}**: ê´€ë ¨ ë‚´ìš© ë°œê²¬\n"
                        # ê´€ë ¨ ë¬¸ì¥ ì°¾ê¸°
                        sentences = memory_data['text'].split('.')
                        relevant_sentences = [s for s in sentences if any(keyword in s.lower() for keyword in pdf_question.lower().split())]
                        if relevant_sentences:
                            answer += f"   - {relevant_sentences[0][:100]}...\n\n"
                
                if answer == "**ê¸°ì–µëœ PDF ë¶„ì„ ê²°ê³¼:**\n\n":
                    answer += "ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            elif "ê³µí†µ" in pdf_question or "ëª¨ë“ " in pdf_question:
                # ê³µí†µ ì£¼ì œ ì°¾ê¸°
                answer = f"**ê¸°ì–µëœ PDF ê³µí†µ ì£¼ì œ ë¶„ì„:**\n\n"
                answer += f"ì´ {len(pdf_names)}ê°œì˜ PDFê°€ ê¸°ì–µë˜ê³  ìˆìŠµë‹ˆë‹¤:\n"
                for pdf_name in pdf_names:
                    answer += f"â€¢ {pdf_name}\n"
                
                # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë¶„ì„
                common_keywords = ["AI", "ì¸ê³µì§€ëŠ¥", "ê¸°ìˆ ", "ì‹œìŠ¤í…œ", "ë°ì´í„°", "ë¶„ì„", "ê°œë°œ", "í”„ë¡œê·¸ë¨"]
                found_keywords = []
                
                for keyword in common_keywords:
                    if any(keyword in memory_data['text'] for memory_data in st.session_state.multiple_pdfs_memory.values()):
                        found_keywords.append(keyword)
                
                if found_keywords:
                    answer += f"\n**ê³µí†µ í‚¤ì›Œë“œ:** {', '.join(found_keywords)}"
                else:
                    answer += "\nê³µí†µ ì£¼ì œë¥¼ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤."
            
            else:
                # ì¼ë°˜ì ì¸ ì§ˆë¬¸
                context = f"ê¸°ì–µëœ PDFë“¤:\n{all_pdf_content[:2000]}..."
                answer = generate_answer(pdf_question, context, "gpt-4o")
            
            # ëŒ€í™” ê¸°ë¡ ì €ì¥
            history_entry = {
                'question': pdf_question,
                'answer': answer,
                'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
                'type': f"ë‹¤ì¤‘ PDF ì§ˆë¬¸ ({len(pdf_names)}ê°œ PDF)"
            }
            st.session_state.history.append(history_entry)
            
            st.subheader("ğŸ¤– ë‹µë³€")
            st.write(answer)
else:
    st.info("ë¨¼ì € PDF íŒŒì¼ë“¤ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”. ì—…ë¡œë“œëœ PDFë“¤ì€ ìë™ìœ¼ë¡œ ê¸°ì–µë©ë‹ˆë‹¤.")

# ëŒ€í™” íˆìŠ¤í† ë¦¬
if st.session_state.history:
    st.markdown('<div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 1.5rem; border-radius: 15px; margin: 2rem 0; text-align: center;">ğŸ“š ëŒ€í™” íˆìŠ¤í† ë¦¬</div>', unsafe_allow_html=True)
    
    for i, entry in enumerate(reversed(st.session_state.history[-5:]), 1):
        with st.expander(f"ì§ˆë¬¸ {i}: {entry['question'][:50]}..."):
            st.markdown(f"""
            <div style="background: #2d2d2d; color: #ffffff; padding: 1.5rem; border-radius: 15px; margin: 1rem 0; border-left: 4px solid #667eea;">
                <h4>ğŸ’¬ ì§ˆë¬¸</h4>
                <p>{entry['question']}</p>
                
                <h4>ğŸ¤– ë‹µë³€</h4>
                <p>{entry['answer']}</p>
                
                <div style="margin-top: 1rem; padding: 0.5rem; background: #f8f9fa; border-radius: 8px; text-align: center;">
                    <small>â° {entry['timestamp']} | ğŸ“„ {entry.get('type', 'ì¼ë°˜ ì§ˆë¬¸')}</small>
                </div>
            </div>
            """, unsafe_allow_html=True)

# ì´ˆê¸°í™” ë²„íŠ¼
if st.button("ğŸ—‘ï¸ ëª¨ë“  ë°ì´í„° ì´ˆê¸°í™”"):
    st.session_state.multiple_pdfs_memory = {}
    st.session_state.history = []
    st.session_state.docs = None
    st.session_state.embs = None
    st.session_state.pdf_text = ""
    st.success("ì´ˆê¸°í™” ì™„ë£Œ!")
    st.rerun()
