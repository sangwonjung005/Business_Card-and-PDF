import os
import streamlit as st
from PyPDF2 import PdfReader
import numpy as np
import time
import re
import requests
from PIL import Image
import pytesseract
import json
from typing import Dict, List, Optional
import base64
from io import BytesIO

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="GPT-OSS Only Business Card OCR & PDF Assistant",
    page_icon="ğŸ’¼",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ë°ì´í„° ì €ì¥ íŒŒì¼ ê²½ë¡œ
DATA_DIR = "app_data"
BUSINESS_CARDS_FILE = os.path.join(DATA_DIR, "business_cards.json")
CONVERSATION_FILE = os.path.join(DATA_DIR, "conversation_history.json")

# ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(DATA_DIR, exist_ok=True)

def save_data_to_file(data, filename):
    """ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        st.error(f"ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return False

def load_data_from_file(filename, default_value):
    """JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
    try:
        if os.path.exists(filename):
            with open(filename, 'r', encoding='utf-8') as f:
                return json.load(f)
        return default_value
    except Exception as e:
        st.warning(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return default_value

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (íŒŒì¼ì—ì„œ ë¡œë“œ)
if "business_cards" not in st.session_state:
    st.session_state.business_cards = load_data_from_file(BUSINESS_CARDS_FILE, [])
if "pdf_docs" not in st.session_state:
    st.session_state.pdf_docs = None
if "conversation_history" not in st.session_state:
    st.session_state.conversation_history = load_data_from_file(CONVERSATION_FILE, [])

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    .main {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: #ffffff;
    }
    
    .card {
        background: rgba(255, 255, 255, 0.1);
        border-radius: 15px;
        padding: 20px;
        margin: 10px 0;
        backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.2);
    }
    
    .business-card {
        background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        border-radius: 15px;
        padding: 20px;
        margin: 10px 0;
        color: white;
        box-shadow: 0 8px 32px rgba(0,0,0,0.3);
    }
    
    .pdf-section {
        background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
        border-radius: 15px;
        padding: 20px;
        margin: 10px 0;
        color: #333;
    }
    
    .stButton > button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 25px;
        padding: 10px 25px;
        font-weight: bold;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(0,0,0,0.3);
    }
</style>
""", unsafe_allow_html=True)

def call_gpt_oss_api(prompt: str) -> str:
    """AI API í˜¸ì¶œ - ì—¬ëŸ¬ ëª¨ë¸ ì‹œë„ (Gemma í¬í•¨)"""
    
    # í•µì‹¬ ëª¨ë¸ë“¤ (GPT-OSS + Gemma + ê¸°ë³¸)
    models = [
        # GPT-OSS ëª¨ë¸ë“¤ (ìš°ì„ ìˆœìœ„)
        "openai/gpt-oss-20b",
        "openai/gpt-oss-120b",
        # Gemma ëª¨ë¸ë“¤ (ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸)
        "google/gemma-3-270m",
        "google/gemma-2b",
        "google/gemma-7b",
        # ê¸°ë³¸ ì•ˆì • ëª¨ë¸
        "microsoft/DialoGPT-medium"
    ]
    
    for model in models:
        try:
            st.write(f"ğŸ”„ Trying {model}...")
            
            # Hugging Face API ì‹œë„
            API_URL = f"https://api-inference.huggingface.co/models/{model}"
            headers = {"Content-Type": "application/json"}
            
            # ëª¨ë¸ë³„ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ í˜•ì‹
            if "gpt-oss" in model:
                payload = {
                    "inputs": prompt,
                    "parameters": {
                        "max_new_tokens": 500,
                        "temperature": 0.3,
                        "do_sample": True
                    }
                }
            elif "gemma" in model:
                # Gemma ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ í˜•ì‹
                payload = {
                    "inputs": f"<start_of_turn>user\n{prompt}<end_of_turn>\n<start_of_turn>model\n",
                    "parameters": {
                        "max_new_tokens": 500,
                        "temperature": 0.3,
                        "do_sample": True,
                        "top_p": 0.9
                    }
                }
            elif "dialo" in model.lower():
                # DialoGPT ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ í˜•ì‹
                payload = {
                    "inputs": prompt,
                    "parameters": {
                        "max_new_tokens": 300,
                        "temperature": 0.7,
                        "do_sample": True,
                        "pad_token_id": 50256
                    }
                }
            else:
                # ì¼ë°˜ GPT ëª¨ë¸ë“¤
                payload = {
                    "inputs": prompt,
                    "parameters": {
                        "max_new_tokens": 200,
                        "temperature": 0.7,
                        "do_sample": True
                    }
                }
            
            response = requests.post(API_URL, headers=headers, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if isinstance(result, list) and len(result) > 0:
                    generated_text = result[0].get('generated_text', '')
                    # í”„ë¡¬í”„íŠ¸ ì œê±°
                    if prompt in generated_text:
                        generated_text = generated_text.replace(prompt, '').strip()
                    # Gemma íŠ¹ë³„ ì²˜ë¦¬
                    if "gemma" in model:
                        # Gemma ì‘ë‹µì—ì„œ ëª¨ë¸ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                        if "<start_of_turn>model\n" in generated_text:
                            generated_text = generated_text.split("<start_of_turn>model\n")[-1]
                        if "<end_of_turn>" in generated_text:
                            generated_text = generated_text.split("<end_of_turn>")[0]
                    st.success(f"âœ… {model} ì„±ê³µ!")
                    return generated_text
                else:
                    st.warning(f"âš ï¸ {model}: ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜")
            else:
                st.warning(f"âš ï¸ {model}: HTTP {response.status_code}")
                
        except Exception as e:
            st.warning(f"âš ï¸ {model}: {str(e)}")
            continue
    
    # Hugging Face ì‹¤íŒ¨ì‹œ Ollama ì‹œë„ (ì„ íƒì‚¬í•­)
    st.info("ğŸ”„ Hugging Face ì‹¤íŒ¨. Ollama Gemma ì‹œë„...")
    
    ollama_models = ["gemma3:270m", "gemma2:2b", "gemma2:7b"]
    
    for ollama_model in ollama_models:
        try:
            st.write(f"ğŸ”„ Trying Ollama {ollama_model}...")
            
            # Ollama API í˜¸ì¶œ
            ollama_url = "http://localhost:11434/api/generate"
            ollama_payload = {
                "model": ollama_model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.3,
                    "top_p": 0.9,
                    "num_predict": 500
                }
            }
            
            response = requests.post(ollama_url, json=ollama_payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                generated_text = result.get('response', '')
                if generated_text:
                    st.success(f"âœ… Ollama {ollama_model} ì„±ê³µ!")
                    return generated_text
                else:
                    st.warning(f"âš ï¸ Ollama {ollama_model}: ë¹ˆ ì‘ë‹µ")
            else:
                st.warning(f"âš ï¸ Ollama {ollama_model}: HTTP {response.status_code}")
                
        except Exception as e:
            st.warning(f"âš ï¸ Ollama {ollama_model}: {str(e)}")
            continue
    
    # ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì‘ë‹µ
    st.error("âŒ ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨.")
    return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ëª¨ë¸ë“¤ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

def extract_business_card_info(image):
    """ëª…í•¨ ì´ë¯¸ì§€ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
    try:
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (PIL ì‚¬ìš©)
        gray_image = image.convert('L')
        
        # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
        width, height = gray_image.size
        if width > 1200:
            ratio = 1200 / width
            new_width = 1200
            new_height = int(height * ratio)
            gray_image = gray_image.resize((new_width, new_height), Image.Resampling.LANCZOS)
        
        # OCR ì‹¤í–‰
        text = pytesseract.image_to_string(gray_image, lang='kor+eng')
        
        # GPT-OSSë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ êµ¬ì¡°í™”
        structured_info = structure_business_card_info(text)
        
        return structured_info
        
    except Exception as e:
        st.error(f"ëª…í•¨ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def structure_business_card_info(raw_text):
    """GPT-OSSë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…í•¨ ì •ë³´ë¥¼ êµ¬ì¡°í™”"""
    try:
        prompt = f"""
ë‹¤ìŒ ëª…í•¨ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:

{raw_text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:
{{
    "name": "ì´ë¦„",
    "title": "ì§ì±…",
    "company": "íšŒì‚¬ëª…",
    "email": "ì´ë©”ì¼",
    "phone": "ì „í™”ë²ˆí˜¸",
    "mobile": "íœ´ëŒ€í°",
    "address": "ì£¼ì†Œ",
    "website": "ì›¹ì‚¬ì´íŠ¸",
    "department": "ë¶€ì„œ"
}}

ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° nullë¡œ í‘œì‹œí•˜ì„¸ìš”.
"""
        
        # GPT-OSS API í˜¸ì¶œ
        response = call_gpt_oss_api(prompt)
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = response[json_start:json_end]
                return json.loads(json_str)
        except:
            pass
        
        # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
        return {
            "name": "ì¶”ì¶œ ì‹¤íŒ¨",
            "title": None,
            "company": None,
            "email": None,
            "phone": None,
            "mobile": None,
            "address": None,
            "website": None,
            "department": None,
            "raw_text": raw_text,
            "gpt_oss_response": response
        }
        
    except Exception as e:
        return {"error": str(e), "raw_text": raw_text}

def read_pdf(file) -> str:
    """PDF ì½ê¸°"""
    text = ""
    reader = PdfReader(file)
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text + "\n"
    return text

def chunk_text(text: str, chunk_size: int = 200, overlap: int = 50):
    """í…ìŠ¤íŠ¸ ì²­í‚¹"""
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        chunk = " ".join(words[start : start + chunk_size])
        chunks.append(chunk)
        start += chunk_size - overlap
    return chunks

def get_context(question: str, docs: list) -> str:
    """ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
    if not docs:
        return ""
    
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­
    question_words = set(question.lower().split())
    best_chunks = []
    
    for i, doc in enumerate(docs[:5]):
        doc_words = set(doc.lower().split())
        overlap = len(question_words.intersection(doc_words))
        if overlap > 0:
            best_chunks.append(doc)
    
    return "\n\n".join(best_chunks[:3]) if best_chunks else docs[0] if docs else ""

def generate_answer(question: str, context: str) -> str:
    """GPT-OSSë¥¼ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±"""
    try:
        if context:
            prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ì°¸ê³  ì •ë³´:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
        else:
            prompt = question
        
        response = call_gpt_oss_api(prompt)
        return response
        
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"

# ë©”ì¸ UI
st.title("ğŸ’¼ AI Business Card OCR & PDF Assistant")
st.markdown("**GPT-OSS + Gemma + ì˜¤í”ˆì†ŒìŠ¤ AI** - ëª…í•¨ OCRê³¼ PDF ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")

# íƒ­ ìƒì„±
tab1, tab2, tab3 = st.tabs(["ğŸ“‡ ëª…í•¨ OCR", "ğŸ“„ PDF RAG", "ğŸ’¬ ëŒ€í™” ê¸°ë¡"])

with tab1:
    st.markdown('<div class="card">', unsafe_allow_html=True)
    st.header("ğŸ“‡ ëª…í•¨ OCR (AI)")
    st.write("ëª…í•¨ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ë©´ GPT-OSS, Gemma ë˜ëŠ” ê¸°íƒ€ AIê°€ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.")
    
    uploaded_image = st.file_uploader(
        "ëª…í•¨ ì´ë¯¸ì§€ ì—…ë¡œë“œ",
        type=['png', 'jpg', 'jpeg'],
        help="ëª…í•¨ ì‚¬ì§„ì„ ì—…ë¡œë“œí•˜ì„¸ìš”"
    )
    
    if uploaded_image is not None:
        image = Image.open(uploaded_image)
        st.image(image, caption="ì—…ë¡œë“œëœ ëª…í•¨", use_column_width=True)
        
        if st.button("ğŸ” AIë¡œ ëª…í•¨ ì •ë³´ ì¶”ì¶œ", type="primary"):
            with st.spinner("AIê°€ ëª…í•¨ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                card_info = extract_business_card_info(image)
                
                if card_info and "error" not in card_info:
                    # ëª…í•¨ ì •ë³´ë¥¼ ì„¸ì…˜ì— ì €ì¥
                    card_info["timestamp"] = time.strftime("%Y-%m-%d %H:%M:%S")
                    st.session_state.business_cards.append(card_info)
                    
                    # íŒŒì¼ì— ì˜êµ¬ ì €ì¥
                    save_data_to_file(st.session_state.business_cards, BUSINESS_CARDS_FILE)
                    
                    # ê²°ê³¼ í‘œì‹œ
                    st.markdown('<div class="business-card">', unsafe_allow_html=True)
                    st.subheader("ğŸ“‹ AI ì¶”ì¶œ ê²°ê³¼")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        if card_info.get("name"):
                            st.write(f"**ì´ë¦„:** {card_info['name']}")
                        if card_info.get("title"):
                            st.write(f"**ì§ì±…:** {card_info['title']}")
                        if card_info.get("company"):
                            st.write(f"**íšŒì‚¬:** {card_info['company']}")
                        if card_info.get("department"):
                            st.write(f"**ë¶€ì„œ:** {card_info['department']}")
                    
                    with col2:
                        if card_info.get("email"):
                            st.write(f"**ì´ë©”ì¼:** {card_info['email']}")
                        if card_info.get("phone"):
                            st.write(f"**ì „í™”:** {card_info['phone']}")
                        if card_info.get("mobile"):
                            st.write(f"**íœ´ëŒ€í°:** {card_info['mobile']}")
                        if card_info.get("website"):
                            st.write(f"**ì›¹ì‚¬ì´íŠ¸:** {card_info['website']}")
                    
                    if card_info.get("address"):
                        st.write(f"**ì£¼ì†Œ:** {card_info['address']}")
                    
                    st.markdown('</div>', unsafe_allow_html=True)
                    
                    st.success("âœ… AIê°€ ëª…í•¨ ì •ë³´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤!")
                else:
                    st.error("AI ëª…í•¨ ì •ë³´ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    # ì €ì¥ëœ ëª…í•¨ ëª©ë¡
    if st.session_state.business_cards:
        st.subheader("ğŸ“š ì €ì¥ëœ ëª…í•¨ ëª©ë¡")
        for i, card in enumerate(st.session_state.business_cards):
            with st.expander(f"ëª…í•¨ {i+1}: {card.get('name', 'Unknown')} - {card.get('company', 'Unknown')}"):
                st.json(card)
    st.markdown('</div>', unsafe_allow_html=True)

with tab2:
    st.markdown('<div class="pdf-section">', unsafe_allow_html=True)
    st.header("ğŸ“„ PDF RAG (AI)")
    st.write("PDFë¥¼ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•˜ë©´ GPT-OSS, Gemma ë˜ëŠ” ê¸°íƒ€ AIê°€ ë‹µë³€í•©ë‹ˆë‹¤.")
    
    uploaded_pdf = st.file_uploader(
        "PDF íŒŒì¼ ì—…ë¡œë“œ",
        type=['pdf'],
        help="PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”"
    )
    
    if uploaded_pdf is not None:
        with st.spinner("PDFë¥¼ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            pdf_text = read_pdf(uploaded_pdf)
            if pdf_text:
                chunks = chunk_text(pdf_text)
                st.session_state.pdf_docs = chunks
                st.success(f"âœ… PDF ì²˜ë¦¬ ì™„ë£Œ! {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
                
                # PDF ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                with st.expander("ğŸ“– PDF ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°"):
                    st.text_area("PDF ë‚´ìš©", pdf_text[:1000] + "...", height=200, disabled=True)
    
    # ì§ˆë¬¸ ì…ë ¥
    if st.session_state.pdf_docs:
        st.subheader("ğŸ’¬ PDFì— ëŒ€í•´ AIì—ê²Œ ì§ˆë¬¸í•˜ê¸°")
        
        question = st.text_input(
            "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
            placeholder="PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."
        )
        
        if st.button("ğŸ¤– AI ë‹µë³€ ìƒì„±", type="primary") and question:
            with st.spinner("AIê°€ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                context = get_context(question, st.session_state.pdf_docs)
                
                # GPT-OSS ë‹µë³€ ìƒì„±
                answer = generate_answer(question, context)
                
                # ëŒ€í™” ê¸°ë¡ì— ì €ì¥
                conversation_entry = {
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "question": question,
                    "answer": answer,
                    "context": context[:200] + "..." if len(context) > 200 else context
                }
                st.session_state.conversation_history.append(conversation_entry)
                
                # íŒŒì¼ì— ì˜êµ¬ ì €ì¥
                save_data_to_file(st.session_state.conversation_history, CONVERSATION_FILE)
                
                # ë‹µë³€ í‘œì‹œ
                st.markdown('<div class="card">', unsafe_allow_html=True)
                st.subheader("ğŸ¤– AI ë‹µë³€")
                st.write(answer)
                
                if context:
                    with st.expander("ğŸ“„ ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸"):
                        st.text(context)
                
                st.markdown('</div>', unsafe_allow_html=True)
    else:
        st.info("ğŸ“„ PDFë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    st.markdown('</div>', unsafe_allow_html=True)

with tab3:
    st.markdown('<div class="card">', unsafe_allow_html=True)
    st.header("ğŸ’¬ AI ëŒ€í™” ê¸°ë¡")
    
    if st.session_state.conversation_history:
        for i, entry in enumerate(reversed(st.session_state.conversation_history)):
            with st.expander(f"ëŒ€í™” {len(st.session_state.conversation_history) - i}: {entry['question'][:50]}..."):
                st.write(f"**ì§ˆë¬¸:** {entry['question']}")
                st.write(f"**AI ë‹µë³€:** {entry['answer']}")
                st.write(f"**ì‹œê°„:** {entry['timestamp']}")
                
                if entry.get('context'):
                    with st.expander("ì»¨í…ìŠ¤íŠ¸"):
                        st.text(entry['context'])
    else:
        st.info("ì•„ì§ AI ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"):
            st.session_state.conversation_history = []
            # íŒŒì¼ì—ì„œë„ ì‚­ì œ
            try:
                if os.path.exists(CONVERSATION_FILE):
                    os.remove(CONVERSATION_FILE)
                st.success("ëŒ€í™” ê¸°ë¡ì´ ì™„ì „íˆ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤!")
            except:
                st.warning("íŒŒì¼ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            st.rerun()
    
    with col2:
        if st.button("ğŸ—‘ï¸ ëª…í•¨ ë°ì´í„° ì´ˆê¸°í™”"):
            st.session_state.business_cards = []
            # íŒŒì¼ì—ì„œë„ ì‚­ì œ
            try:
                if os.path.exists(BUSINESS_CARDS_FILE):
                    os.remove(BUSINESS_CARDS_FILE)
                st.success("ëª…í•¨ ë°ì´í„°ê°€ ì™„ì „íˆ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤!")
            except:
                st.warning("íŒŒì¼ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            st.rerun()
    st.markdown('</div>', unsafe_allow_html=True)

# ì‚¬ì´ë“œë°” í†µê³„
with st.sidebar:
    st.header("ğŸ“Š AI í†µê³„")
    
    # ëª…í•¨ í†µê³„
    st.subheader("ğŸ“‡ ëª…í•¨")
    st.metric("ì €ì¥ëœ ëª…í•¨", len(st.session_state.business_cards))
    
    # PDF í†µê³„
    st.subheader("ğŸ“„ PDF")
    if st.session_state.pdf_docs:
        st.metric("ì²­í¬ ìˆ˜", len(st.session_state.pdf_docs))
    else:
        st.metric("ì²­í¬ ìˆ˜", 0)
    
    # ëŒ€í™” í†µê³„
    st.subheader("ğŸ’¬ ëŒ€í™”")
    st.metric("AI ëŒ€í™” ìˆ˜", len(st.session_state.conversation_history))
    
    st.markdown("---")
    
    # ë°ì´í„° ê´€ë¦¬ ì •ë³´
    st.header("ğŸ’¾ ë°ì´í„° ê´€ë¦¬")
    st.info(f"""
    **ğŸ“ ì €ì¥ ìœ„ì¹˜:** `{DATA_DIR}/`
    - ëª…í•¨ ë°ì´í„°: `business_cards.json`
    - ëŒ€í™” ê¸°ë¡: `conversation_history.json`
    
    **ğŸ”„ ìë™ ì €ì¥:** ëª¨ë“  ë°ì´í„°ëŠ” ìë™ìœ¼ë¡œ íŒŒì¼ì— ì €ì¥ë©ë‹ˆë‹¤.
    **ğŸ“± ì˜êµ¬ ë³´ì¡´:** ë¸Œë¼ìš°ì €ë¥¼ ë‹«ì•„ë„ ë°ì´í„°ê°€ ìœ ì§€ë©ë‹ˆë‹¤.
    """)
    
    st.markdown("---")
    
    # ê¸°ëŠ¥ ì„¤ëª…
    st.header("ğŸ”§ AI ê¸°ëŠ¥")
    st.write("""
    **ğŸ“‡ ëª…í•¨ OCR:**
    - ëª…í•¨ ì´ë¯¸ì§€ ì—…ë¡œë“œ
    - AI ì •ë³´ ì¶”ì¶œ (GPT-OSS, Gemma, DialoGPT)
    - êµ¬ì¡°í™”ëœ ë°ì´í„° ì €ì¥
    
    **ğŸ“„ PDF RAG:**
    - PDF ë¬¸ì„œ ì—…ë¡œë“œ
    - í…ìŠ¤íŠ¸ ì²­í‚¹
    - AI ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ
    
    **ğŸ’¬ ëŒ€í™” ê¸°ë¡:**
    - AI ì§ˆë¬¸-ë‹µë³€ íˆìŠ¤í† ë¦¬
    - ì»¨í…ìŠ¤íŠ¸ ì¶”ì 
    - ë©”ëª¨ë¦¬ ê´€ë¦¬
    """)
